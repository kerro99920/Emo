"""
app.py - Flaskåç«¯APIï¼Œç”¨äºè¡¨æƒ…è¯†åˆ«
"""

from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
import io
import base64
import os
import sys

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.model import get_model

app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# å…¨å±€å˜é‡
model = None
device = None
transform = None
classes = ['anger', 'fear', 'happy', 'sad', 'surprise']

# ä¸­æ–‡ç¿»è¯‘
class_names_zh = {
    'anger': 'æ„¤æ€’',
    'fear': 'ææƒ§',
    'happy': 'å¿«ä¹',
    'sad': 'æ‚²ä¼¤',
    'surprise': 'æƒŠè®¶'
}

# è¡¨æƒ…emoji
class_emojis = {
    'anger': 'ğŸ˜ ',
    'fear': 'ğŸ˜¨',
    'happy': 'ğŸ˜Š',
    'sad': 'ğŸ˜¢',
    'surprise': 'ğŸ˜²'
}


def load_model(model_path, model_type):
    """åŠ è½½æ¨¡å‹"""
    global model, device, transform

    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæ¨¡å‹
    model = get_model(model_type, num_classes=5, pretrained=False)

    # åŠ è½½æƒé‡
    checkpoint = torch.load(model_path, map_location=device)

    # æ£€æŸ¥æ˜¯å¦æ˜¯é‡åŒ–æ¨¡å‹
    if checkpoint.get('quantized', False):
        print("åŠ è½½é‡åŒ–æ¨¡å‹...")
        model = torch.quantization.quantize_dynamic(
            model,
            {nn.Linear, nn.Conv2d},
            dtype=torch.qint8
        )

    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()

    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((48, 48)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_type}")
    print(f"   è®­ç»ƒæ—¶å‡†ç¡®ç‡: {checkpoint.get('accuracy', 'N/A')}")


def predict_image(image):
    """é¢„æµ‹å›¾ç‰‡"""
    if model is None:
        return None

    # é¢„å¤„ç†
    image_tensor = transform(image).unsqueeze(0).to(device)

    # é¢„æµ‹
    with torch.no_grad():
        output = model(image_tensor)
        probabilities = torch.nn.functional.softmax(output, dim=1)
        confidence, predicted = torch.max(probabilities, 1)

    # è·å–æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
    probs = probabilities[0].cpu().numpy()

    result = {
        'predicted_class': classes[predicted.item()],
        'predicted_class_zh': class_names_zh[classes[predicted.item()]],
        'emoji': class_emojis[classes[predicted.item()]],
        'confidence': float(confidence.item()),
        'probabilities': {
            classes[i]: {
                'en': classes[i],
                'zh': class_names_zh[classes[i]],
                'emoji': class_emojis[classes[i]],
                'probability': float(probs[i])
            }
            for i in range(len(classes))
        }
    }

    return result


@app.route('/')
def index():
    """ä¸»é¡µ"""
    return render_template('index.html')


@app.route('/api/models', methods=['GET'])
def get_available_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    models_dir = 'models'

    if not os.path.exists(models_dir):
        return jsonify({'error': 'æ¨¡å‹æ–‡ä»¶å¤¹ä¸å­˜åœ¨'}), 404

    models = []
    for f in os.listdir(models_dir):
        if f.endswith('.pth'):
            model_path = os.path.join(models_dir, f)
            size_mb = os.path.getsize(model_path) / (1024 * 1024)

            # æ¨æ–­æ¨¡å‹ç±»å‹
            if 'distilled' in f:
                parts = f.replace('distilled_', '').replace('.pth', '').split('_from_')
                model_type = parts[0]
            elif 'quantized' in f:
                model_type = f.replace('best_model_', '').replace('_quantized.pth', '')
            elif 'pruned' in f:
                model_type = f.replace('best_model_', '').split('_pruned_')[0]
            else:
                model_type = f.replace('best_model_', '').replace('final_model_', '').replace('.pth', '')

            models.append({
                'filename': f,
                'model_type': model_type,
                'size_mb': round(size_mb, 2),
                'is_quantized': 'quantized' in f,
                'is_pruned': 'pruned' in f,
                'is_distilled': 'distilled' in f
            })

    return jsonify({'models': models})


@app.route('/api/load_model', methods=['POST'])
def load_model_endpoint():
    """åŠ è½½æŒ‡å®šæ¨¡å‹"""
    data = request.json
    model_filename = data.get('model_filename')
    model_type = data.get('model_type')

    if not model_filename or not model_type:
        return jsonify({'error': 'ç¼ºå°‘å‚æ•°'}), 400

    model_path = os.path.join('models', model_filename)

    if not os.path.exists(model_path):
        return jsonify({'error': 'æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨'}), 404

    try:
        load_model(model_path, model_type)
        return jsonify({
            'success': True,
            'message': f'æ¨¡å‹åŠ è½½æˆåŠŸ: {model_type}',
            'device': str(device)
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/predict', methods=['POST'])
def predict():
    """é¢„æµ‹æ¥å£"""
    if model is None:
        return jsonify({'error': 'è¯·å…ˆåŠ è½½æ¨¡å‹'}), 400

    try:
        # è·å–å›¾ç‰‡æ•°æ®
        if 'file' in request.files:
            # æ–‡ä»¶ä¸Šä¼ 
            file = request.files['file']
            image = Image.open(file.stream).convert('RGB')
        elif 'image' in request.json:
            # Base64ç¼–ç çš„å›¾ç‰‡
            image_data = request.json['image']
            if ',' in image_data:
                image_data = image_data.split(',')[1]
            image_bytes = base64.b64decode(image_data)
            image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
        else:
            return jsonify({'error': 'æœªæä¾›å›¾ç‰‡'}), 400

        # é¢„æµ‹
        result = predict_image(image)

        return jsonify(result)

    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/status', methods=['GET'])
def status():
    """è·å–æœåŠ¡çŠ¶æ€"""
    return jsonify({
        'model_loaded': model is not None,
        'device': str(device) if device else None,
        'classes': classes,
        'classes_zh': class_names_zh
    })


if __name__ == '__main__':
    # é»˜è®¤åŠ è½½ä¸€ä¸ªæ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    default_model = 'models/best_model_resnet18.pth'
    if os.path.exists(default_model):
        try:
            load_model(default_model, 'resnet18')
            print("âœ… é»˜è®¤æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸  é»˜è®¤æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("   è¯·é€šè¿‡APIæ‰‹åŠ¨åŠ è½½æ¨¡å‹")

    print("\n" + "=" * 70)
    print("ğŸš€ FlaskæœåŠ¡å™¨å¯åŠ¨")
    print("=" * 70)
    print("è®¿é—®åœ°å€: http://localhost:5000")
    print("APIæ–‡æ¡£:")
    print("  GET  /api/models       - è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨")
    print("  POST /api/load_model   - åŠ è½½æŒ‡å®šæ¨¡å‹")
    print("  POST /api/predict      - é¢„æµ‹å›¾ç‰‡")
    print("  GET  /api/status       - è·å–æœåŠ¡çŠ¶æ€")
    print("=" * 70)

    app.run(debug=True, host='0.0.0.0', port=5000)