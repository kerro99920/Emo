"""
evaluate.py - æ¨¡å‹è¯„ä¼°è„šæœ¬ï¼ˆæ”¯æŒä¼˜åŒ–å’Œè’¸é¦åçš„æ¨¡å‹ï¼‰
è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
from tqdm import tqdm
import numpy as np
import os
import time
import pandas as pd

from dataset import EmotionDataset
from model import get_model
from utils import (
    plot_confusion_matrix,
    print_classification_report,
    load_checkpoint
)


def load_model_for_evaluation(model_path, model_type, device):
    """
    åŠ è½½æ¨¡å‹ç”¨äºè¯„ä¼°ï¼ˆæ”¯æŒæ™®é€šã€é‡åŒ–ã€å‰ªæã€è’¸é¦æ¨¡å‹ï¼‰

    Args:
        model_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
        model_type: æ¨¡å‹ç±»å‹
        device: è®¾å¤‡

    Returns:
        model: åŠ è½½å¥½çš„æ¨¡å‹
        model_info: æ¨¡å‹ä¿¡æ¯å­—å…¸
    """
    print(f"\nåŠ è½½æ¨¡å‹: {model_path}")

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

    # åŠ è½½checkpoint
    checkpoint = torch.load(model_path, map_location=device)

    # åˆ¤æ–­æ¨¡å‹ç±»å‹
    is_quantized = checkpoint.get('quantized', False)
    is_pruned = checkpoint.get('pruned', False)
    is_distilled = 'distilled' in model_path

    # åˆ›å»ºæ¨¡å‹
    model = get_model(model_type, num_classes=5, pretrained=False)

    # åŠ è½½æƒé‡
    model.load_state_dict(checkpoint['model_state_dict'])

    # å¦‚æœæ˜¯é‡åŒ–æ¨¡å‹ï¼Œæ‰§è¡Œé‡åŒ–
    if is_quantized:
        print("âœ“ æ£€æµ‹åˆ°é‡åŒ–æ¨¡å‹")
        model = torch.quantization.quantize_dynamic(
            model,
            {nn.Linear, nn.Conv2d},
            dtype=torch.qint8
        )

    model = model.to(device)
    model.eval()

    # æ”¶é›†æ¨¡å‹ä¿¡æ¯
    model_info = {
        'model_type': model_type,
        'quantized': is_quantized,
        'pruned': is_pruned,
        'distilled': is_distilled,
        'saved_accuracy': checkpoint.get('accuracy', 'N/A'),
        'saved_epoch': checkpoint.get('epoch', 'N/A'),
        'prune_amount': checkpoint.get('prune_amount', 'N/A')
    }

    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"   æ¨¡å‹ç±»å‹: {model_type}")
    if is_quantized:
        print(f"   âœ“ é‡åŒ–æ¨¡å‹")
    if is_pruned:
        print(f"   âœ“ å‰ªææ¨¡å‹ (å‰ªææ¯”ä¾‹: {checkpoint.get('prune_amount', 'N/A')})")
    if is_distilled:
        print(f"   âœ“ è’¸é¦æ¨¡å‹")
    print(f"   è®­ç»ƒæ—¶å‡†ç¡®ç‡: {checkpoint.get('accuracy', 'N/A')}")

    return model, model_info


def evaluate_model(model_path, model_type, batch_size=64, device='auto', save_results=True):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½

    Args:
        model_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
        model_type: æ¨¡å‹ç±»å‹
        batch_size: æ‰¹æ¬¡å¤§å°
        device: è®¾å¤‡
        save_results: æ˜¯å¦ä¿å­˜ç»“æœ

    Returns:
        results: è¯„ä¼°ç»“æœå­—å…¸
    """

    # è®¾ç½®è®¾å¤‡
    if device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(device)

    print("=" * 70)
    print("ğŸ“Š æ¨¡å‹è¯„ä¼°")
    print("=" * 70)
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"æ¨¡å‹ç±»å‹: {model_type}")
    print(f"è®¾å¤‡: {device}")
    print("=" * 70)

    # æ•°æ®é¢„å¤„ç†ï¼ˆä¸éªŒè¯é›†ç›¸åŒï¼‰
    val_transform = transforms.Compose([
        transforms.Resize((48, 48)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])

    # åŠ è½½éªŒè¯é›†
    print("\nåŠ è½½éªŒè¯é›†...")
    val_dataset = EmotionDataset('../data/val', transform=val_transform)
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True if device.type == 'cuda' else False
    )

    # åŠ è½½æ¨¡å‹
    model, model_info = load_model_for_evaluation(model_path, model_type, device)

    # è·å–æ¨¡å‹å¤§å°
    model_size_mb = os.path.getsize(model_path) / (1024 * 1024)

    # è¯„ä¼°
    print("\n" + "=" * 70)
    print("å¼€å§‹è¯„ä¼°...")
    print("=" * 70)

    all_predictions = []
    all_labels = []
    correct = 0
    total = 0

    # æµ‹é‡æ¨ç†æ—¶é—´
    inference_times = []

    with torch.no_grad():
        pbar = tqdm(val_loader, desc='è¯„ä¼°ä¸­', ncols=100)

        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)

            # æµ‹é‡æ¨ç†æ—¶é—´
            start_time = time.time()
            outputs = model(images)
            inference_time = time.time() - start_time
            inference_times.append(inference_time)

            _, predicted = torch.max(outputs.data, 1)

            # ç»Ÿè®¡
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            # ä¿å­˜é¢„æµ‹ç»“æœ
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            # æ›´æ–°è¿›åº¦æ¡
            accuracy = 100 * correct / total
            pbar.set_postfix({'acc': f'{accuracy:.2f}%'})

    # è®¡ç®—æœ€ç»ˆå‡†ç¡®ç‡
    final_accuracy = 100 * correct / total

    # è®¡ç®—æ¨ç†é€Ÿåº¦
    avg_inference_time = np.mean(inference_times)
    avg_batch_time = avg_inference_time * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
    throughput = batch_size / avg_inference_time  # å›¾ç‰‡/ç§’

    print("\n" + "=" * 70)
    print("ğŸ“ˆ è¯„ä¼°ç»“æœ")
    print("=" * 70)
    print(f"æ€»æ ·æœ¬æ•°: {total}")
    print(f"æ­£ç¡®é¢„æµ‹: {correct}")
    print(f"å‡†ç¡®ç‡: {final_accuracy:.2f}%")
    print(f"\næ€§èƒ½æŒ‡æ ‡:")
    print(f"  æ¨¡å‹å¤§å°: {model_size_mb:.2f} MB")
    print(f"  æ¨ç†é€Ÿåº¦: {avg_batch_time:.2f} ms/batch")
    print(f"  ååé‡: {throughput:.2f} å›¾ç‰‡/ç§’")

    # ç±»åˆ«åç§°
    classes = ['anger', 'fear', 'happy', 'sad', 'surprise']

    # æ‰“å°åˆ†ç±»æŠ¥å‘Š
    print_classification_report(all_labels, all_predictions, classes)

    # ä¿å­˜æ··æ·†çŸ©é˜µ
    if save_results:
        # ç”Ÿæˆä¿å­˜è·¯å¾„ï¼ˆåŒ…å«æ¨¡å‹ç‰¹å¾ï¼‰
        model_name = os.path.basename(model_path).replace('.pth', '')
        cm_path = f'../results/confusion_matrix_{model_name}.png'
        plot_confusion_matrix(all_labels, all_predictions, classes, cm_path)

    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    print("\n" + "=" * 70)
    print("ğŸ“Š å„ç±»åˆ«å‡†ç¡®ç‡")
    print("=" * 70)

    class_correct = [0] * 5
    class_total = [0] * 5

    for label, pred in zip(all_labels, all_predictions):
        class_total[label] += 1
        if label == pred:
            class_correct[label] += 1

    for i, class_name in enumerate(classes):
        if class_total[i] > 0:
            class_acc = 100 * class_correct[i] / class_total[i]
            print(f"{class_name:10s}: {class_acc:.2f}% ({class_correct[i]}/{class_total[i]})")

    print("\n" + "=" * 70)
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print("=" * 70)

    # è¿”å›ç»“æœ
    results = {
        'model_path': model_path,
        'model_type': model_type,
        'accuracy': final_accuracy,
        'model_size_mb': model_size_mb,
        'inference_time_ms': avg_batch_time,
        'throughput': throughput,
        'quantized': model_info['quantized'],
        'pruned': model_info['pruned'],
        'distilled': model_info['distilled'],
        'prune_amount': model_info.get('prune_amount', 'N/A')
    }

    return results


def evaluate_all_models():
    """è¯„ä¼°æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆåŒ…æ‹¬ä¼˜åŒ–å’Œè’¸é¦åçš„ï¼‰"""

    models_dir = '../models'

    if not os.path.exists(models_dir):
        print("âŒ models æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return

    # è·å–æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
    all_model_files = [f for f in os.listdir(models_dir) if f.endswith('.pth')]

    # åˆ†ç±»æ¨¡å‹
    base_models = [f for f in all_model_files if f.startswith('best_model_')]
    quantized_models = [f for f in all_model_files if 'quantized' in f]
    pruned_models = [f for f in all_model_files if 'pruned' in f]
    distilled_models = [f for f in all_model_files if 'distilled' in f]

    print("\n" + "=" * 70)
    print("ğŸ“Š æ‰¹é‡è¯„ä¼°æ‰€æœ‰æ¨¡å‹")
    print("=" * 70)
    print(f"\nå‘ç°æ¨¡å‹:")
    print(f"  åŸºç¡€æ¨¡å‹: {len(base_models)} ä¸ª")
    print(f"  é‡åŒ–æ¨¡å‹: {len(quantized_models)} ä¸ª")
    print(f"  å‰ªææ¨¡å‹: {len(pruned_models)} ä¸ª")
    print(f"  è’¸é¦æ¨¡å‹: {len(distilled_models)} ä¸ª")
    print(f"  æ€»è®¡: {len(all_model_files)} ä¸ª")

    all_results = []

    # è¯„ä¼°æ‰€æœ‰æ¨¡å‹
    for model_file in all_model_files:
        model_path = os.path.join(models_dir, model_file)

        # æ¨æ–­æ¨¡å‹ç±»å‹
        if 'distilled' in model_file:
            # è’¸é¦æ¨¡å‹æ ¼å¼: distilled_studenttype_from_teachertype.pth
            parts = model_file.replace('distilled_', '').replace('.pth', '').split('_from_')
            model_type = parts[0]
        elif 'quantized' in model_file:
            model_type = model_file.replace('best_model_', '').replace('_quantized.pth', '')
        elif 'pruned' in model_file:
            model_type = model_file.replace('best_model_', '').split('_pruned_')[0]
        else:
            model_type = model_file.replace('best_model_', '').replace('final_model_', '').replace('.pth', '')

        try:
            print(f"\n{'='*70}")
            print(f"è¯„ä¼°: {model_file}")
            print(f"{'='*70}")

            result = evaluate_model(model_path, model_type, save_results=True)
            result['model_name'] = model_file
            all_results.append(result)

        except Exception as e:
            print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
            all_results.append({
                'model_name': model_file,
                'model_type': model_type,
                'status': f'Failed: {e}'
            })

    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    generate_comprehensive_report(all_results)

    return all_results


def generate_comprehensive_report(results):
    """ç”Ÿæˆç»¼åˆå¯¹æ¯”æŠ¥å‘Š"""

    # åªä¿ç•™æˆåŠŸçš„ç»“æœ
    successful_results = [r for r in results if 'accuracy' in r]

    if not successful_results:
        print("\nâš ï¸  æ²¡æœ‰æˆåŠŸè¯„ä¼°çš„æ¨¡å‹")
        return

    df = pd.DataFrame(successful_results)

    # æ‰“å°è¯¦ç»†å¯¹æ¯”è¡¨
    print("\n" + "=" * 80)
    print("ğŸ“Š æ‰€æœ‰æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    print("=" * 80)

    display_cols = ['model_name', 'accuracy', 'model_size_mb', 'inference_time_ms', 'throughput']
    print(df[display_cols].to_string(index=False))

    # æŒ‰ç±»åˆ«åˆ†ç»„ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("ğŸ“ˆ åˆ†ç±»ç»Ÿè®¡")
    print("=" * 80)

    # åŸºç¡€æ¨¡å‹
    base_df = df[~df['quantized'] & ~df['pruned'] & ~df['distilled']]
    if not base_df.empty:
        print(f"\nğŸ”¹ åŸºç¡€æ¨¡å‹ ({len(base_df)}ä¸ª):")
        print(f"   å¹³å‡å‡†ç¡®ç‡: {base_df['accuracy'].mean():.2f}%")
        print(f"   å¹³å‡å¤§å°: {base_df['model_size_mb'].mean():.2f} MB")
        print(f"   å¹³å‡é€Ÿåº¦: {base_df['inference_time_ms'].mean():.2f} ms/batch")

    # é‡åŒ–æ¨¡å‹
    quant_df = df[df['quantized']]
    if not quant_df.empty:
        print(f"\nğŸ”¹ é‡åŒ–æ¨¡å‹ ({len(quant_df)}ä¸ª):")
        print(f"   å¹³å‡å‡†ç¡®ç‡: {quant_df['accuracy'].mean():.2f}%")
        print(f"   å¹³å‡å¤§å°: {quant_df['model_size_mb'].mean():.2f} MB")
        print(f"   å¹³å‡é€Ÿåº¦: {quant_df['inference_time_ms'].mean():.2f} ms/batch")
        if not base_df.empty:
            print(f"   ğŸ“‰ å¤§å°å‹ç¼©: {base_df['model_size_mb'].mean() / quant_df['model_size_mb'].mean():.2f}x")
            print(f"   âš¡ é€Ÿåº¦æå‡: {base_df['inference_time_ms'].mean() / quant_df['inference_time_ms'].mean():.2f}x")

    # å‰ªææ¨¡å‹
    prune_df = df[df['pruned']]
    if not prune_df.empty:
        print(f"\nğŸ”¹ å‰ªææ¨¡å‹ ({len(prune_df)}ä¸ª):")
        print(f"   å¹³å‡å‡†ç¡®ç‡: {prune_df['accuracy'].mean():.2f}%")
        print(f"   å¹³å‡å¤§å°: {prune_df['model_size_mb'].mean():.2f} MB")

    # è’¸é¦æ¨¡å‹
    distill_df = df[df['distilled']]
    if not distill_df.empty:
        print(f"\nğŸ”¹ è’¸é¦æ¨¡å‹ ({len(distill_df)}ä¸ª):")
        print(f"   å¹³å‡å‡†ç¡®ç‡: {distill_df['accuracy'].mean():.2f}%")
        print(f"   å¹³å‡å¤§å°: {distill_df['model_size_mb'].mean():.2f} MB")
        print(f"   å¹³å‡é€Ÿåº¦: {distill_df['inference_time_ms'].mean():.2f} ms/batch")

    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    print("\n" + "=" * 80)
    print("ğŸ† æœ€ä½³æ¨¡å‹")
    print("=" * 80)

    best_acc = df.loc[df['accuracy'].idxmax()]
    smallest = df.loc[df['model_size_mb'].idxmin()]
    fastest = df.loc[df['inference_time_ms'].idxmin()]

    # è®¡ç®—æ•ˆç‡åˆ†æ•°ï¼ˆå‡†ç¡®ç‡ / å¤§å°ï¼‰
    df['efficiency_score'] = df['accuracy'] / df['model_size_mb']
    most_efficient = df.loc[df['efficiency_score'].idxmax()]

    print(f"ğŸ¥‡ æœ€é«˜å‡†ç¡®ç‡: {best_acc['model_name']}")
    print(f"   å‡†ç¡®ç‡: {best_acc['accuracy']:.2f}%")
    print(f"   å¤§å°: {best_acc['model_size_mb']:.2f} MB")

    print(f"\nğŸ¥ˆ æœ€å°æ¨¡å‹: {smallest['model_name']}")
    print(f"   å¤§å°: {smallest['model_size_mb']:.2f} MB")
    print(f"   å‡†ç¡®ç‡: {smallest['accuracy']:.2f}%")

    print(f"\nğŸ¥‰ æœ€å¿«æ¨ç†: {fastest['model_name']}")
    print(f"   é€Ÿåº¦: {fastest['inference_time_ms']:.2f} ms/batch")
    print(f"   å‡†ç¡®ç‡: {fastest['accuracy']:.2f}%")

    print(f"\nğŸ’¡ æœ€é«˜æ•ˆç‡: {most_efficient['model_name']}")
    print(f"   æ•ˆç‡åˆ†æ•°: {most_efficient['efficiency_score']:.2f}")
    print(f"   å‡†ç¡®ç‡: {most_efficient['accuracy']:.2f}%")
    print(f"   å¤§å°: {most_efficient['model_size_mb']:.2f} MB")

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    csv_path = '../results/comprehensive_evaluation.csv'
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {csv_path}")


def compare_optimization_effects():
    """å¯¹æ¯”ä¼˜åŒ–æ•ˆæœï¼ˆåŸºç¡€ vs é‡åŒ– vs å‰ªæ vs è’¸é¦ï¼‰"""

    models_dir = '../models'

    print("\n" + "=" * 70)
    print("ğŸ”¬ ä¼˜åŒ–æ•ˆæœå¯¹æ¯”åˆ†æ")
    print("=" * 70)

    # æŸ¥æ‰¾é…å¯¹çš„æ¨¡å‹ï¼ˆåŸºç¡€æ¨¡å‹å’Œå®ƒçš„ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    base_models = {}

    for f in os.listdir(models_dir):
        if f.startswith('best_model_') and f.endswith('.pth') and 'quantized' not in f and 'pruned' not in f:
            model_type = f.replace('best_model_', '').replace('.pth', '')
            base_models[model_type] = {
                'base': f,
                'quantized': f'best_model_{model_type}_quantized.pth' if os.path.exists(
                    os.path.join(models_dir, f'best_model_{model_type}_quantized.pth')) else None,
                'pruned': None  # å¯ä»¥æ‰©å±•
            }

    comparison_results = []

    for model_type, files in base_models.items():
        print(f"\nåˆ†ææ¨¡å‹: {model_type}")
        print("-" * 70)

        results = {}

        # è¯„ä¼°åŸºç¡€æ¨¡å‹
        if files['base']:
            base_path = os.path.join(models_dir, files['base'])
            base_result = evaluate_model(base_path, model_type, save_results=False)
            results['base'] = base_result
            print(f"  åŸºç¡€æ¨¡å‹: {base_result['accuracy']:.2f}% | {base_result['model_size_mb']:.2f} MB")

        # è¯„ä¼°é‡åŒ–æ¨¡å‹
        if files['quantized']:
            quant_path = os.path.join(models_dir, files['quantized'])
            quant_result = evaluate_model(quant_path, model_type, save_results=False)
            results['quantized'] = quant_result
            print(f"  é‡åŒ–æ¨¡å‹: {quant_result['accuracy']:.2f}% | {quant_result['model_size_mb']:.2f} MB")

            # è®¡ç®—å˜åŒ–
            if 'base' in results:
                acc_change = quant_result['accuracy'] - results['base']['accuracy']
                size_ratio = results['base']['model_size_mb'] / quant_result['model_size_mb']
                speed_ratio = results['base']['inference_time_ms'] / quant_result['inference_time_ms']

                print(f"  ğŸ“Š ä¼˜åŒ–æ•ˆæœ:")
                print(f"     å‡†ç¡®ç‡å˜åŒ–: {acc_change:+.2f}%")
                print(f"     å¤§å°å‹ç¼©: {size_ratio:.2f}x")
                print(f"     é€Ÿåº¦æå‡: {speed_ratio:.2f}x")

        comparison_results.append({
            'model_type': model_type,
            'results': results
        })

    print("\n" + "=" * 70)
    print("âœ… å¯¹æ¯”åˆ†æå®Œæˆ")
    print("=" * 70)


def predict_single_image(model_path, model_type, image_path, device='auto'):
    """
    å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œé¢„æµ‹

    Args:
        model_path: æ¨¡å‹æƒé‡è·¯å¾„
        model_type: æ¨¡å‹ç±»å‹
        image_path: å›¾ç‰‡è·¯å¾„
        device: è®¾å¤‡
    """
    from PIL import Image

    # è®¾ç½®è®¾å¤‡
    if device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(device)

    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((48, 48)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])

    # åŠ è½½æ¨¡å‹
    model, model_info = load_model_for_evaluation(model_path, model_type, device)

    # åŠ è½½å¹¶é¢„å¤„ç†å›¾ç‰‡
    image = Image.open(image_path).convert('RGB')
    image_tensor = transform(image).unsqueeze(0).to(device)

    # é¢„æµ‹
    start_time = time.time()
    with torch.no_grad():
        output = model(image_tensor)
        probabilities = torch.nn.functional.softmax(output, dim=1)
        confidence, predicted = torch.max(probabilities, 1)
    inference_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’

    # ç±»åˆ«åç§°
    classes = ['anger', 'fear', 'happy', 'sad', 'surprise']

    print("\n" + "=" * 70)
    print("ğŸ–¼ï¸  å•å¼ å›¾ç‰‡é¢„æµ‹ç»“æœ")
    print("=" * 70)
    print(f"å›¾ç‰‡è·¯å¾„: {image_path}")
    print(f"æ¨¡å‹: {os.path.basename(model_path)}")
    print(f"é¢„æµ‹ç±»åˆ«: {classes[predicted.item()]}")
    print(f"ç½®ä¿¡åº¦: {confidence.item()*100:.2f}%")
    print(f"æ¨ç†æ—¶é—´: {inference_time:.2f} ms")
    print("\næ‰€æœ‰ç±»åˆ«æ¦‚ç‡:")
    for i, class_name in enumerate(classes):
        prob = probabilities[0][i].item()*100
        bar = 'â–ˆ' * int(prob / 2)
        print(f"  {class_name:10s}: {prob:5.2f}% {bar}")
    print("=" * 70)

    return classes[predicted.item()], confidence.item()


if __name__ == "__main__":
    import sys

    print("\n" + "=" * 70)
    print("ğŸ“Š æ¨¡å‹è¯„ä¼°å·¥å…·ï¼ˆå¢å¼ºç‰ˆï¼‰")
    print("=" * 70)
    print("\nå¯ç”¨åŠŸèƒ½:")
    print("  [1] è¯„ä¼°å•ä¸ªæ¨¡å‹")
    print("  [2] è¯„ä¼°æ‰€æœ‰æ¨¡å‹ï¼ˆåŒ…æ‹¬ä¼˜åŒ–åçš„ï¼‰")
    print("  [3] å¯¹æ¯”ä¼˜åŒ–æ•ˆæœï¼ˆåŸºç¡€ vs é‡åŒ–ï¼‰")
    print("  [4] é¢„æµ‹å•å¼ å›¾ç‰‡")

    choice = input("\nè¯·é€‰æ‹©åŠŸèƒ½ (1-4): ").strip()

    if choice == '1':
        # è¯„ä¼°å•ä¸ªæ¨¡å‹
        models_dir = '../models'
        if os.path.exists(models_dir):
            model_files = [f for f in os.listdir(models_dir) if f.endswith('.pth')]

            print("\nå¯ç”¨æ¨¡å‹:")
            for i, model in enumerate(model_files, 1):
                size_mb = os.path.getsize(os.path.join(models_dir, model)) / (1024 * 1024)
                print(f"  [{i}] {model} ({size_mb:.2f} MB)")

            model_idx = int(input("\né€‰æ‹©æ¨¡å‹ç¼–å·: ")) - 1
            model_path = os.path.join(models_dir, model_files[model_idx])

            # æ¨æ–­æ¨¡å‹ç±»å‹
            if 'distilled' in model_files[model_idx]:
                parts = model_files[model_idx].replace('distilled_', '').replace('.pth', '').split('_from_')
                model_type = parts[0]
            elif 'quantized' in model_files[model_idx]:
                model_type = model_files[model_idx].replace('best_model_', '').replace('_quantized.pth', '')
            elif 'pruned' in model_files[model_idx]:
                model_type = model_files[model_idx].replace('best_model_', '').split('_pruned_')[0]
            else:
                model_type = model_files[model_idx].replace('best_model_', '').replace('final_model_', '').replace('.pth', '')

            evaluate_model(model_path, model_type)
        else:
            print("âŒ models æ–‡ä»¶å¤¹ä¸å­˜åœ¨")

    elif choice == '2':
        # è¯„ä¼°æ‰€æœ‰æ¨¡å‹
        evaluate_all_models()

    elif choice == '3':
        # å¯¹æ¯”ä¼˜åŒ–æ•ˆæœ
        compare_optimization_effects()

    elif choice == '4':
        # é¢„æµ‹å•å¼ å›¾ç‰‡
        models_dir = '../models'
        if os.path.exists(models_dir):
            model_files = [f for f in os.listdir(models_dir) if f.endswith('.pth')]

            print("\nå¯ç”¨æ¨¡å‹:")
            for i, model in enumerate(model_files, 1):
                print(f"  [{i}] {model}")

            model_idx = int(input("\né€‰æ‹©æ¨¡å‹ç¼–å·: ")) - 1
            model_path = os.path.join(models_dir, model_files[model_idx])

            # æ¨æ–­æ¨¡å‹ç±»å‹
            if 'distilled' in model_files[model_idx]:
                parts = model_files[model_idx].replace('distilled_', '').replace('.pth', '').split('_from_')
                model_type = parts[0]
            else:
                model_type = model_files[model_idx].replace('best_model_', '').replace('_quantized.pth', '').replace('.pth', '')

            image_path = input("è¾“å…¥å›¾ç‰‡è·¯å¾„: ").strip()
            predict_single_image(model_path, model_type, image_path)
        else:
            print("âŒ models æ–‡ä»¶å¤¹ä¸å­˜åœ¨")

    print("\n" + "=" * 70)
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print("=" * 70)