"""
optimize_distill.py - æ¨¡å‹ä¼˜åŒ–ä¸çŸ¥è¯†è’¸é¦
åŒ…å«æ¨¡å‹é‡åŒ–ã€å‰ªæå’ŒçŸ¥è¯†è’¸é¦åŠŸèƒ½
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
from tqdm import tqdm
import os
import time

from dataset import EmotionDataset
from model import get_model
from utils import save_checkpoint


# ==================== æ¨¡å‹é‡åŒ– ====================

def quantize_model(model_path, model_type, save_path=None):
    """
    åŠ¨æ€é‡åŒ–æ¨¡å‹ - å°†FP32è½¬æ¢ä¸ºINT8ï¼Œå‡å°æ¨¡å‹å¤§å°

    ä¼˜ç‚¹:
        - æ¨¡å‹å¤§å°å‡å°‘75%ï¼ˆçº¦4å€å‹ç¼©ï¼‰
        - æ¨ç†é€Ÿåº¦æå‡2-4å€ï¼ˆCPUä¸Šï¼‰
        - å‡†ç¡®ç‡æŸå¤±å¾ˆå°ï¼ˆé€šå¸¸<1%ï¼‰

    é€‚ç”¨åœºæ™¯:
        - CPUéƒ¨ç½²
        - ç§»åŠ¨ç«¯éƒ¨ç½²
        - å­˜å‚¨ç©ºé—´æœ‰é™

    Args:
        model_path: åŸå§‹æ¨¡å‹è·¯å¾„
        model_type: æ¨¡å‹ç±»å‹
        save_path: é‡åŒ–åæ¨¡å‹ä¿å­˜è·¯å¾„
    """
    print("\n" + "=" * 70)
    print("ğŸ”§ æ¨¡å‹é‡åŒ–ï¼ˆDynamic Quantizationï¼‰")
    print("=" * 70)

    # åŠ è½½åŸå§‹æ¨¡å‹
    print(f"åŠ è½½æ¨¡å‹: {model_path}")
    model = get_model(model_type, num_classes=5, pretrained=False)
    checkpoint = torch.load(model_path, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # è·å–åŸå§‹æ¨¡å‹å¤§å°
    original_size = os.path.getsize(model_path) / (1024 * 1024)  # MB

    # åŠ¨æ€é‡åŒ–
    print("\næ­£åœ¨é‡åŒ–æ¨¡å‹...")
    quantized_model = torch.quantization.quantize_dynamic(
        model,
        {nn.Linear, nn.Conv2d},  # é‡åŒ–çº¿æ€§å±‚å’Œå·ç§¯å±‚
        dtype=torch.qint8
    )

    # ä¿å­˜é‡åŒ–æ¨¡å‹
    if save_path is None:
        save_path = model_path.replace('.pth', '_quantized.pth')

    torch.save({
        'model_state_dict': quantized_model.state_dict(),
        'model_type': model_type,
        'quantized': True
    }, save_path)

    quantized_size = os.path.getsize(save_path) / (1024 * 1024)  # MB

    print("\n" + "=" * 70)
    print("âœ… é‡åŒ–å®Œæˆï¼")
    print("=" * 70)
    print(f"åŸå§‹æ¨¡å‹å¤§å°: {original_size:.2f} MB")
    print(f"é‡åŒ–åå¤§å°: {quantized_size:.2f} MB")
    print(f"å‹ç¼©æ¯”: {original_size/quantized_size:.2f}x")
    print(f"ä¿å­˜è·¯å¾„: {save_path}")
    print("\nå»ºè®®:")
    print("  - åœ¨CPUä¸Šè¿è¡Œ evaluate.py æµ‹è¯•é‡åŒ–æ¨¡å‹çš„å‡†ç¡®ç‡")
    print("  - é‡åŒ–æ¨¡å‹ç‰¹åˆ«é€‚åˆç§»åŠ¨ç«¯å’Œè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²")

    return quantized_model, save_path


def prune_model(model_path, model_type, prune_amount=0.3, save_path=None):
    """
    æ¨¡å‹å‰ªæ - ç§»é™¤ä¸é‡è¦çš„æƒé‡ï¼Œå‡å°æ¨¡å‹å¤§å°

    ä¼˜ç‚¹:
        - å‡å°‘å‚æ•°é‡å’Œè®¡ç®—é‡
        - åŠ å¿«æ¨ç†é€Ÿåº¦
        - å‡å°æ¨¡å‹å¤§å°

    é€‚ç”¨åœºæ™¯:
        - éœ€è¦è½»é‡çº§æ¨¡å‹
        - è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²

    Args:
        model_path: åŸå§‹æ¨¡å‹è·¯å¾„
        model_type: æ¨¡å‹ç±»å‹
        prune_amount: å‰ªææ¯”ä¾‹ï¼ˆ0.3è¡¨ç¤ºå‰ªæ‰30%çš„æƒé‡ï¼‰
        save_path: å‰ªæåæ¨¡å‹ä¿å­˜è·¯å¾„
    """
    print("\n" + "=" * 70)
    print(f"âœ‚ï¸  æ¨¡å‹å‰ªæï¼ˆPruning - {prune_amount*100}%ï¼‰")
    print("=" * 70)

    import torch.nn.utils.prune as prune

    # åŠ è½½åŸå§‹æ¨¡å‹
    print(f"åŠ è½½æ¨¡å‹: {model_path}")
    model = get_model(model_type, num_classes=5, pretrained=False)
    checkpoint = torch.load(model_path, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])

    # ç»Ÿè®¡åŸå§‹å‚æ•°
    original_params = sum(p.numel() for p in model.parameters())

    # å¯¹æ‰€æœ‰å·ç§¯å±‚å’Œçº¿æ€§å±‚è¿›è¡Œå‰ªæ
    print(f"\næ­£åœ¨å‰ªæ {prune_amount*100}% çš„æƒé‡...")
    parameters_to_prune = []

    for name, module in model.named_modules():
        if isinstance(module, (nn.Conv2d, nn.Linear)):
            parameters_to_prune.append((module, 'weight'))

    # å…¨å±€éç»“æ„åŒ–å‰ªæ
    prune.global_unstructured(
        parameters_to_prune,
        pruning_method=prune.L1Unstructured,
        amount=prune_amount,
    )

    # ç§»é™¤å‰ªæçš„é‡å‚æ•°åŒ–ï¼ˆä½¿å‰ªææ°¸ä¹…åŒ–ï¼‰
    for module, param_name in parameters_to_prune:
        prune.remove(module, param_name)

    # ç»Ÿè®¡å‰ªæåçš„å‚æ•°
    pruned_params = sum(p.numel() for p in model.parameters())
    zero_params = sum((p == 0).sum().item() for p in model.parameters())

    # ä¿å­˜å‰ªææ¨¡å‹
    if save_path is None:
        save_path = model_path.replace('.pth', f'_pruned_{int(prune_amount*100)}.pth')

    torch.save({
        'model_state_dict': model.state_dict(),
        'model_type': model_type,
        'pruned': True,
        'prune_amount': prune_amount
    }, save_path)

    print("\n" + "=" * 70)
    print("âœ… å‰ªæå®Œæˆï¼")
    print("=" * 70)
    print(f"åŸå§‹å‚æ•°é‡: {original_params:,}")
    print(f"å‰ªæåå‚æ•°é‡: {pruned_params:,}")
    print(f"é›¶å‚æ•°æ•°é‡: {zero_params:,} ({zero_params/pruned_params*100:.2f}%)")
    print(f"ä¿å­˜è·¯å¾„: {save_path}")
    print("\nå»ºè®®:")
    print("  - å‰ªæåçš„æ¨¡å‹éœ€è¦å¾®è°ƒï¼ˆfine-tuneï¼‰ä»¥æ¢å¤å‡†ç¡®ç‡")
    print("  - è¿è¡Œ evaluate.py æµ‹è¯•å‰ªææ¨¡å‹çš„å‡†ç¡®ç‡")

    return model, save_path


# ==================== çŸ¥è¯†è’¸é¦ ====================

class DistillationLoss(nn.Module):
    """
    çŸ¥è¯†è’¸é¦æŸå¤±å‡½æ•°

    ç»“åˆäº†:
        1. å­¦ç”Ÿæ¨¡å‹ä¸çœŸå®æ ‡ç­¾çš„äº¤å‰ç†µæŸå¤±
        2. å­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹è¾“å‡ºçš„KLæ•£åº¦æŸå¤±
    """
    def __init__(self, temperature=3.0, alpha=0.5):
        """
        Args:
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è½¯æ ‡ç­¾çš„å¹³æ»‘ç¨‹åº¦
            alpha: å¹³è¡¡ç³»æ•°ï¼Œ0-1ä¹‹é—´ï¼Œæ§åˆ¶ä¸¤ç§æŸå¤±çš„æƒé‡
        """
        super(DistillationLoss, self).__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.ce_loss = nn.CrossEntropyLoss()
        self.kl_loss = nn.KLDivLoss(reduction='batchmean')

    def forward(self, student_logits, teacher_logits, labels):
        """
        è®¡ç®—è’¸é¦æŸå¤±

        Args:
            student_logits: å­¦ç”Ÿæ¨¡å‹è¾“å‡º
            teacher_logits: æ•™å¸ˆæ¨¡å‹è¾“å‡º
            labels: çœŸå®æ ‡ç­¾
        """
        # ç¡¬æ ‡ç­¾æŸå¤±ï¼ˆå­¦ç”Ÿ vs çœŸå®æ ‡ç­¾ï¼‰
        hard_loss = self.ce_loss(student_logits, labels)

        # è½¯æ ‡ç­¾æŸå¤±ï¼ˆå­¦ç”Ÿ vs æ•™å¸ˆï¼‰
        student_soft = nn.functional.log_softmax(student_logits / self.temperature, dim=1)
        teacher_soft = nn.functional.softmax(teacher_logits / self.temperature, dim=1)
        soft_loss = self.kl_loss(student_soft, teacher_soft) * (self.temperature ** 2)

        # æ€»æŸå¤±
        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss

        return total_loss, hard_loss, soft_loss


def knowledge_distillation(
    teacher_model_path,
    teacher_model_type,
    student_model_type,
    num_epochs=15,
    batch_size=64,
    learning_rate=0.001,
    temperature=3.0,
    alpha=0.5,
    device='auto'
):
    """
    çŸ¥è¯†è’¸é¦è®­ç»ƒ

    å°†å¤§æ¨¡å‹ï¼ˆæ•™å¸ˆï¼‰çš„çŸ¥è¯†ä¼ é€’ç»™å°æ¨¡å‹ï¼ˆå­¦ç”Ÿï¼‰

    å…¸å‹ç»„åˆ:
        - æ•™å¸ˆ: ResNet50/ResNet34  â†’  å­¦ç”Ÿ: ResNet18
        - æ•™å¸ˆ: ResNet18           â†’  å­¦ç”Ÿ: MobileNet
        - æ•™å¸ˆ: EfficientNet       â†’  å­¦ç”Ÿ: MobileNet

    Args:
        teacher_model_path: æ•™å¸ˆæ¨¡å‹è·¯å¾„
        teacher_model_type: æ•™å¸ˆæ¨¡å‹ç±»å‹
        student_model_type: å­¦ç”Ÿæ¨¡å‹ç±»å‹
        num_epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        learning_rate: å­¦ä¹ ç‡
        temperature: è’¸é¦æ¸©åº¦ï¼ˆè¶Šå¤§è¶Šå¹³æ»‘ï¼‰
        alpha: ç¡¬æ ‡ç­¾æŸå¤±æƒé‡ï¼ˆ0-1ï¼‰
        device: è®¾å¤‡
    """

    # è®¾ç½®è®¾å¤‡
    if device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(device)

    print("\n" + "=" * 70)
    print("ğŸ“ çŸ¥è¯†è’¸é¦è®­ç»ƒ")
    print("=" * 70)
    print(f"æ•™å¸ˆæ¨¡å‹: {teacher_model_type}")
    print(f"å­¦ç”Ÿæ¨¡å‹: {student_model_type}")
    print(f"è®¾å¤‡: {device}")
    print(f"è®­ç»ƒè½®æ•°: {num_epochs}")
    print(f"æ¸©åº¦å‚æ•°: {temperature}")
    print(f"Alphaå‚æ•°: {alpha}")
    print("=" * 70)

    # æ•°æ®é¢„å¤„ç†
    train_transform = transforms.Compose([
        transforms.Resize((48, 48)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize((48, 48)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])

    # åŠ è½½æ•°æ®é›†
    print("\nåŠ è½½æ•°æ®é›†...")
    train_dataset = EmotionDataset('../data/train', transform=train_transform)
    val_dataset = EmotionDataset('../data/val', transform=val_transform)

    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True,
        num_workers=4, pin_memory=True if device.type == 'cuda' else False
    )

    val_loader = DataLoader(
        val_dataset, batch_size=batch_size, shuffle=False,
        num_workers=4, pin_memory=True if device.type == 'cuda' else False
    )

    # åŠ è½½æ•™å¸ˆæ¨¡å‹
    print("\nåŠ è½½æ•™å¸ˆæ¨¡å‹...")
    teacher_model = get_model(teacher_model_type, num_classes=5, pretrained=False)
    checkpoint = torch.load(teacher_model_path, map_location=device)
    teacher_model.load_state_dict(checkpoint['model_state_dict'])
    teacher_model = teacher_model.to(device)
    teacher_model.eval()  # æ•™å¸ˆæ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œä¸æ›´æ–°å‚æ•°

    print(f"âœ… æ•™å¸ˆæ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   éªŒè¯å‡†ç¡®ç‡: {checkpoint.get('accuracy', 'N/A')}")

    # åˆ›å»ºå­¦ç”Ÿæ¨¡å‹
    print("\nåˆ›å»ºå­¦ç”Ÿæ¨¡å‹...")
    student_model = get_model(student_model_type, num_classes=5, pretrained=True)
    student_model = student_model.to(device)

    # ç»Ÿè®¡å‚æ•°é‡
    teacher_params = sum(p.numel() for p in teacher_model.parameters())
    student_params = sum(p.numel() for p in student_model.parameters())

    print(f"\næ¨¡å‹å‚æ•°å¯¹æ¯”:")
    print(f"  æ•™å¸ˆæ¨¡å‹: {teacher_params:,} å‚æ•°")
    print(f"  å­¦ç”Ÿæ¨¡å‹: {student_params:,} å‚æ•°")
    print(f"  å‹ç¼©æ¯”: {teacher_params/student_params:.2f}x")

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    distillation_criterion = DistillationLoss(temperature=temperature, alpha=alpha)
    optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=3
    )

    # è®­ç»ƒå†å²
    history = {
        'train_loss': [],
        'train_hard_loss': [],
        'train_soft_loss': [],
        'train_acc': [],
        'val_acc': []
    }

    best_val_acc = 0.0

    # å¼€å§‹è®­ç»ƒ
    print("\n" + "=" * 70)
    print("å¼€å§‹è’¸é¦è®­ç»ƒ...")
    print("=" * 70)

    start_time = time.time()

    for epoch in range(num_epochs):
        print(f"\nEpoch [{epoch+1}/{num_epochs}]")
        print("-" * 70)

        # è®­ç»ƒé˜¶æ®µ
        student_model.train()
        running_loss = 0.0
        running_hard_loss = 0.0
        running_soft_loss = 0.0
        correct = 0
        total = 0

        pbar = tqdm(train_loader, desc='è®­ç»ƒä¸­', ncols=100)

        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)

            # æ•™å¸ˆæ¨¡å‹æ¨ç†ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
            with torch.no_grad():
                teacher_logits = teacher_model(images)

            # å­¦ç”Ÿæ¨¡å‹æ¨ç†
            optimizer.zero_grad()
            student_logits = student_model(images)

            # è®¡ç®—è’¸é¦æŸå¤±
            loss, hard_loss, soft_loss = distillation_criterion(
                student_logits, teacher_logits, labels
            )

            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()

            # ç»Ÿè®¡
            running_loss += loss.item()
            running_hard_loss += hard_loss.item()
            running_soft_loss += soft_loss.item()

            _, predicted = torch.max(student_logits.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{100 * correct / total:.2f}%'
            })

        avg_loss = running_loss / len(train_loader)
        avg_hard_loss = running_hard_loss / len(train_loader)
        avg_soft_loss = running_soft_loss / len(train_loader)
        train_acc = 100 * correct / total

        # éªŒè¯é˜¶æ®µ
        student_model.eval()
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            pbar = tqdm(val_loader, desc='éªŒè¯ä¸­', ncols=100)

            for images, labels in pbar:
                images, labels = images.to(device), labels.to(device)
                outputs = student_model(images)
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

                pbar.set_postfix({
                    'acc': f'{100 * val_correct / val_total:.2f}%'
                })

        val_acc = 100 * val_correct / val_total

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(val_acc)

        # è®°å½•å†å²
        history['train_loss'].append(avg_loss)
        history['train_hard_loss'].append(avg_hard_loss)
        history['train_soft_loss'].append(avg_soft_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)

        # æ‰“å°ç»“æœ
        print(f"\nç»“æœ:")
        print(f"  è®­ç»ƒ - æ€»Loss: {avg_loss:.4f}, ç¡¬Loss: {avg_hard_loss:.4f}, "
              f"è½¯Loss: {avg_soft_loss:.4f}, Acc: {train_acc:.2f}%")
        print(f"  éªŒè¯ - Acc: {val_acc:.2f}%")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            save_path = f'../models/distilled_{student_model_type}_from_{teacher_model_type}.pth'
            save_checkpoint(
                student_model, optimizer, epoch+1, avg_loss, val_acc, save_path
            )
            print(f"  âœ“ æ–°çš„æœ€ä½³æ¨¡å‹ï¼éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")

    # è®­ç»ƒå®Œæˆ
    total_time = time.time() - start_time

    print("\n" + "=" * 70)
    print("âœ… çŸ¥è¯†è’¸é¦è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)
    print(f"æ€»è®­ç»ƒæ—¶é—´: {total_time/60:.2f} åˆ†é’Ÿ")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    print(f"\nå¯¹æ¯”:")
    print(f"  æ•™å¸ˆæ¨¡å‹åŸå§‹å‡†ç¡®ç‡: {checkpoint.get('accuracy', 'N/A')}")
    print(f"  å­¦ç”Ÿæ¨¡å‹è’¸é¦åå‡†ç¡®ç‡: {best_val_acc:.2f}%")
    print(f"  å‚æ•°é‡å‹ç¼©æ¯”: {teacher_params/student_params:.2f}x")

    return student_model, history


# ==================== ä¸»å‡½æ•° ====================

if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("ğŸ› ï¸  æ¨¡å‹ä¼˜åŒ–ä¸çŸ¥è¯†è’¸é¦å·¥å…·")
    print("=" * 70)
    print("\nå¯ç”¨åŠŸèƒ½:")
    print("  [1] æ¨¡å‹é‡åŒ–ï¼ˆDynamic Quantizationï¼‰")
    print("  [2] æ¨¡å‹å‰ªæï¼ˆPruningï¼‰")
    print("  [3] çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰")
    print("  [4] å…¨éƒ¨æ‰§è¡Œï¼ˆé‡åŒ– + å‰ªæï¼‰")

    choice = input("\nè¯·é€‰æ‹©åŠŸèƒ½ (1-4): ").strip()

    if choice == '1':
        # æ¨¡å‹é‡åŒ–
        print("\nå½“å‰å¯ç”¨çš„æ¨¡å‹:")
        models_dir = '../models'
        if os.path.exists(models_dir):
            models = [f for f in os.listdir(models_dir) if f.startswith('best_model_') and f.endswith('.pth')]
            for i, model in enumerate(models, 1):
                print(f"  [{i}] {model}")

            model_idx = int(input("\né€‰æ‹©è¦é‡åŒ–çš„æ¨¡å‹ç¼–å·: ")) - 1
            model_path = os.path.join(models_dir, models[model_idx])
            model_type = models[model_idx].replace('best_model_', '').replace('.pth', '')

            quantize_model(model_path, model_type)
        else:
            print("âŒ models æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")

    elif choice == '2':
        # æ¨¡å‹å‰ªæ
        print("\nå½“å‰å¯ç”¨çš„æ¨¡å‹:")
        models_dir = '../models'
        if os.path.exists(models_dir):
            models = [f for f in os.listdir(models_dir) if f.startswith('best_model_') and f.endswith('.pth')]
            for i, model in enumerate(models, 1):
                print(f"  [{i}] {model}")

            model_idx = int(input("\né€‰æ‹©è¦å‰ªæçš„æ¨¡å‹ç¼–å·: ")) - 1
            model_path = os.path.join(models_dir, models[model_idx])
            model_type = models[model_idx].replace('best_model_', '').replace('.pth', '')

            prune_amount = float(input("è¾“å…¥å‰ªææ¯”ä¾‹ (0.1-0.5ï¼Œæ¨è0.3): "))

            prune_model(model_path, model_type, prune_amount)
        else:
            print("âŒ models æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")

    elif choice == '3':
        # çŸ¥è¯†è’¸é¦
        print("\næ¨èçš„æ•™å¸ˆ-å­¦ç”Ÿç»„åˆ:")
        print("  [1] ResNet50 â†’ ResNet18")
        print("  [2] ResNet34 â†’ MobileNet")
        print("  [3] EfficientNet â†’ MobileNet")
        print("  [4] ResNet18 â†’ MobileNet")
        print("  [5] è‡ªå®šä¹‰")

        combo_choice = input("\né€‰æ‹©ç»„åˆ (1-5): ").strip()

        combinations = {
            '1': ('resnet50', 'resnet18'),
            '2': ('resnet34', 'mobilenet'),
            '3': ('efficientnet', 'mobilenet'),
            '4': ('resnet18', 'mobilenet')
        }

        if combo_choice in combinations:
            teacher_type, student_type = combinations[combo_choice]
        else:
            teacher_type = input("è¾“å…¥æ•™å¸ˆæ¨¡å‹ç±»å‹: ").strip()
            student_type = input("è¾“å…¥å­¦ç”Ÿæ¨¡å‹ç±»å‹: ").strip()

        teacher_path = f'../models/best_model_{teacher_type}.pth'

        if os.path.exists(teacher_path):
            knowledge_distillation(
                teacher_model_path=teacher_path,
                teacher_model_type=teacher_type,
                student_model_type=student_type,
                num_epochs=15,
                temperature=3.0,
                alpha=0.5
            )
        else:
            print(f"âŒ æ•™å¸ˆæ¨¡å‹ä¸å­˜åœ¨: {teacher_path}")
            print("è¯·å…ˆè®­ç»ƒæ•™å¸ˆæ¨¡å‹")

    elif choice == '4':
        # å…¨éƒ¨æ‰§è¡Œ
        print("\nå°†å¯¹æ‰€æœ‰å·²è®­ç»ƒçš„æ¨¡å‹æ‰§è¡Œé‡åŒ–å’Œå‰ªæ...")
        models_dir = '../models'
        if os.path.exists(models_dir):
            models = [f for f in os.listdir(models_dir) if f.startswith('best_model_') and f.endswith('.pth')]

            for model_file in models:
                model_path = os.path.join(models_dir, model_file)
                model_type = model_file.replace('best_model_', '').replace('.pth', '')

                print(f"\nå¤„ç†æ¨¡å‹: {model_type}")
                quantize_model(model_path, model_type)
                prune_model(model_path, model_type, prune_amount=0.3)
        else:
            print("âŒ models æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")

    print("\n" + "=" * 70)
    print("âœ… å…¨éƒ¨å®Œæˆï¼")
    print("=" * 70)